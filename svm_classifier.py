# -*- coding: utf-8 -*-
"""SVM_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1npvZ6hHaa4Bqw059lHiCoVcbQGkoGiBV

**SUPPORT VECTOR MACHINE (SVM) CLASSIFIER**
"""

''' svm first coined in 1960 and later in 1990 it has been improvised. It is capable of performing classification, regression
and outlier detection. It can also perform non linear classification. It's effective in high dimension spaces. It's still effective
in a case where the no. of dimension is greater than the no. of samples. One of the disadvantages of the svm is that it doesn't
provide the probability directly, these are calculated using filefold cross validation technique. Its main function is to segregate
or separate the given data in the best possilbe way.
-In svm we use these three types of kernals;
1): linear kernal
2): polynomial kernal
3): Radial basis kernal function

-svm is specific to supervised learning. it works on the past data and makes future prediction as output.
-supervised learning further divides into two parts; classification and regression.
-Hyperplane or decision boundary should be optimal in svm. If it is not optimal then there's a high chance of missclassification.
-The regularization parameters or lambda is the parameter where it figures out that if we have bias or overfitting of the data. By applying
the svm it naturally avoid you the overfitting and bias issue.
-svm is supervised learning algorithm. It is mainly used for classification but can also be used for regression problems.
SVR(support vector regressor) is used for regression.
-svm can be used for calssifying the non linear data by using the kernal trick. Kernal trick basically means to transform the data
from one dimension into another dimensions so that the hyperplane or decision boundary can be drawn nicely. So svm can be used for the
non linear data where data is mixed and cannot be separated, you just have to used the kernal trick.
-svm is basically draw a decision boundary between two classes in order to separate them from eachother.
-those nearest points where the doted lines are draw are called the support vectors. And that's why we call it the support vector 
machine.
-the optimum hyperplane has the maximum distance from the support vectors
-svm is good to use if we have less datasets.
 '''

# Step 1:
# Import all the respective libraries
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn import metrics

# Step 2:
#Loading the data
d = datasets.load_breast_cancer()

#Now to explore or check that what is inside the data? how it looks like?

print(d)  #it gives output(target) in binary form(0 or 1)

print(d['target'])  #target is in binomial form (0  and 1). We have only these two possibilities here

# Step 3: Clean the data. Data is already clean

# Step 4: 
# Split of train and test data


x_train, x_test, y_train, y_test = train_test_split(d.data, d.target,test_size =0.4, random_state=209)

# Step 5:
# Create the model with support vector classifier(SVC) using the linear kernel trick
model = svm.SVC(kernel = 'linear')

# Step 6:
# Train the model
#For training we are gonna use the fit method where we pass the x train and y train to train the data.
#Basically fit method is used to train the data
model.fit(x_train,y_train)

# Step 7:
# Prediction of the model
prediction = model.predict(x_test) 
print(prediction)

# Step 8:
# Evaluation of the model
accuracy=metrics.accuracy_score(y_test, y_pred = prediction)  #It gives more than 92% accuracy score
print(accuracy)

#Now to check the precision score
precision = metrics.precision_score(y_test, y_pred = prediction)  #It gives more than 93% precision score
print(precision)

#Now to check the recall score
recall = metrics.recall_score(y_test, y_pred = prediction)  #It gives more than 94% recall score
print(recall)

#Now to print the classification report
c_report = metrics.classification_report(y_test, y_pred = prediction)
print(c_report)